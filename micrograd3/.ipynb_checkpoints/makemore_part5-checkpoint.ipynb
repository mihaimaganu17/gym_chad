{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "426d3b4a-03d1-47db-b8f2-93cfaaef1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "random.seed(0x1337_b00b)\n",
    "\n",
    "# Context length -> How many characters we take as input for the network to predict\n",
    "# the next\n",
    "block_size = 8\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        with open('names.txt', 'r') as f:\n",
    "            names = f.read().split('\\n')\n",
    "        self.names = names\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "        # Shuffle names in place\n",
    "        random.shuffle(self.names)\n",
    "        # Training set and dev/validation set last index\n",
    "        # First 80% is used for training, 10% percent for validation, 10% for test\n",
    "        train_set_idx = int(0.8 * len(names))\n",
    "        validation_idx = int(0.9 * len(names))\n",
    "        \n",
    "        self.X, self.Y = {}, {}\n",
    "        self.X[\"train\"], self.Y[\"train\"] = self.build_dataset(self.names[:train_set_idx])\n",
    "        self.X[\"valid\"], self.Y[\"valid\"] = self.build_dataset(self.names[train_set_idx:validation_idx])\n",
    "        self.X[\"test\"], self.Y[\"test\"] = self.build_dataset(self.names[validation_idx:])\n",
    "        \n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Build vocabulary\n",
    "        vocab = []\n",
    "        for name in self.names:\n",
    "            vocab += name\n",
    "        self.vocab = sorted(set(vocab))\n",
    "        \n",
    "        # Build mapping from letter to integer id and for id to letter\n",
    "        # Leave the `0` key for `.` (dot) which new treat as a null / terminating char\n",
    "        self.itos = { i+1:l for i, l in enumerate(self.vocab)}\n",
    "        self.itos[0] = '.'\n",
    "        # Build the inverse mapping -> from character to integer id\n",
    "        self.stoi = { l:i for i, l in self.itos.items()}\n",
    "                \n",
    "\n",
    "    def build_dataset(self, words):\n",
    "        global block_size\n",
    "        # Inputs\n",
    "        X = []\n",
    "        # Targets\n",
    "        Y = []\n",
    "        \n",
    "        # For each name\n",
    "        for word in words:\n",
    "            # The start is an empty new context (which contains our designed dot special character)\n",
    "            context = [0] * block_size\n",
    "            # For each character in the name (adding dot as a stopping token)\n",
    "            for ch in word + '.':\n",
    "                # We add the current context and as an input to the dataset\n",
    "                X.append(context)\n",
    "                # Get the index of the current character and add it as a target for a potential\n",
    "                # generated new character that could follow this context\n",
    "                idx_ch = self.stoi[ch]\n",
    "                Y.append(idx_ch)\n",
    "                # Slide the context window and add the new character to it\n",
    "                context = context[1:] + [idx_ch]\n",
    "    \n",
    "        X = torch.Tensor(X).long()\n",
    "        Y = torch.Tensor(Y).long()\n",
    "        return (X, Y)\n",
    "\n",
    "\n",
    "    def dataset_demo(self, split, count = 10):\n",
    "        for i, p in zip(self.X[split][:count], self.Y[split][:count]):\n",
    "            print(''.join([self.itos[c.item()] for c in i]), \"-->\", self.itos[p.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6adc7777-bd4d-438a-ba95-7d43e52438f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> l\n",
      ".......l --> a\n",
      "......la --> t\n",
      ".....lat --> a\n",
      "....lata --> s\n",
      "...latas --> h\n",
      "..latash --> a\n",
      ".latasha --> .\n",
      "........ --> k\n",
      ".......k --> y\n",
      "......ky --> n\n",
      ".....kyn --> s\n",
      "....kyns --> l\n",
      "...kynsl --> e\n",
      "..kynsle --> e\n",
      ".kynslee --> .\n",
      "........ --> a\n",
      ".......a --> b\n",
      "......ab --> c\n",
      ".....abc --> d\n",
      "....abcd --> e\n",
      "...abcde --> .\n",
      "........ --> n\n",
      ".......n --> a\n",
      "......na --> d\n",
      ".....nad --> e\n",
      "....nade --> l\n",
      "...nadel --> y\n",
      "..nadely --> n\n",
      ".nadelyn --> .\n"
     ]
    }
   ],
   "source": [
    "d = Dataset()\n",
    "d.dataset_demo(\"train\", count = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "db1bd7b3-b094-44db-8360-90239492530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuring the code\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        kaimin_scaling = fan_in**0.5\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / kaimin_scaling\n",
    "        if bias:\n",
    "            # Bias initializing with zeros or small floats?\n",
    "            self.bias = torch.zeros(fan_out)\n",
    "            # self.bias = torch.rand(fan_out, generator=g) * 0.1\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "\n",
    "    def __call__(self, x_in):\n",
    "        self.out = x_in @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        params = [self.weights]\n",
    "        if self.bias is not None:\n",
    "            params += [self.bias]\n",
    "        return params\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, size, eps=1e-05, momentum=0.1):\n",
    "        \"\"\"Batch normalization layers defined accoding to the paper with the same name\n",
    "        and torch docs\n",
    "\n",
    "        Args:\n",
    "            size: size of the batch and implictily this layer\n",
    "            eps: a small variable to control that we are not dividing by zero\n",
    "            momentum: The amount that each training iteration affects the final running\n",
    "                mean and std used in model inference at production time. \n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        # Controls if we are using the running mean and std when inferencing (in prod)\n",
    "        # or we are computing it from the batch (training)\n",
    "        self.training = True\n",
    "        # Training parameters that get updated by the backward pass\n",
    "        self.gamma = torch.ones(size)\n",
    "        self.beta = torch.zeros(size)\n",
    "        # Extra variables used to control the behaviour of the normalisation\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # Buffers (torch naming) that are not part of the backward pass and training\n",
    "        self.running_mean = torch.zeros((1, size))\n",
    "        self.running_std = torch.ones((1, size))\n",
    "\n",
    "\n",
    "    def __call__(self, x_in):\n",
    "        # Compute the mean and the std for the input\n",
    "        if self.training:\n",
    "            in_mean = x_in.mean(0, keepdim=True)\n",
    "            in_std = x_in.std(0, keepdim=True)\n",
    "        else:\n",
    "            in_mean = self.running_mean\n",
    "            in_std = self.running_std\n",
    "\n",
    "        # Normalize the layer\n",
    "        norm = (x_in - in_mean) / torch.sqrt(in_std + self.eps)\n",
    "        # Compute the batch norm\n",
    "        self.out = self.gamma * norm + self.beta\n",
    "\n",
    "        # If we are training, we need to update the running mean and std\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1.-self.momentum) * self.running_mean \\\n",
    "                    + self.momentum * in_mean\n",
    "                self.running_std = (1.-self.momentum) * self.running_std \\\n",
    "                    + self.momentum * in_std\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, n_embeddings, emb_size):\n",
    "        # Create a 2 dimensional embedding vector for the number of embeddings, where each\n",
    "        # element is represented by an embedding size\n",
    "        self.weights = torch.randn((n_embeddings, emb_size))\n",
    "\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        # Indexing into the embedding vector\n",
    "        self.out = self.weights[IX]\n",
    "        return self.out\n",
    "        \n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "\n",
    "\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        # n represents the amount of elements to be flattened by this layer in the\n",
    "        # last dimension\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        assert T % self.n == 0\n",
    "        # Flattens the given input tensor `x`, given the first dimension shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "\n",
    "        # If the new batch dimension T//self.n is one, we just remove it\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "            \n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        # A container that calls modules sequentially\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x\n",
    "        for layer in self.layers:\n",
    "            self.out = layer(self.out)\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = [p for layer in self.layers for p in layer.parameters()]\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9bf543f4-53c8-4647-abe8-924cb1561e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  22397\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27\n",
    "emb_size = 10\n",
    "n_hidden = 68\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, emb_size),\n",
    "    FlattenConsecutive(2), Linear(emb_size * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident by scaling down the weights\n",
    "    # layers[-1].weights *= 0.1\n",
    "    # When we have BatchNorm1d as the last layer ,we need to scale the gamma because\n",
    "    # we do not have weigths, but gamma effectively acts like weights\n",
    "    seq.layers[-1].weights *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(\"Number of parameters: \", sum([p.nelement() for p in parameters]))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c22716f8-9d04-4682-a65a-98e44e75c239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  7,  1],\n",
       "        [ 0,  0,  0,  0,  0, 10,  1, 14],\n",
       "        [ 0,  0,  0,  0,  1, 14,  3,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  5,  2]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect 4 examples of random indexes from the train set\n",
    "idxs = torch.randint(0, d.X['train'].shape[0], (4,))\n",
    "Xb, Yb = d.X['train'][idxs], d.Y['train'][idxs]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9bb42621-16ff-43e3-b0fd-1c761b5aa60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (4, 8, 10)\n",
      "FlattenConsecutive : (4, 4, 20)\n",
      "Linear : (4, 4, 68)\n",
      "BatchNorm1d : (4, 4, 68)\n",
      "Tanh : (4, 4, 68)\n",
      "FlattenConsecutive : (4, 2, 136)\n",
      "Linear : (4, 2, 68)\n",
      "BatchNorm1d : (4, 2, 68)\n",
      "Tanh : (4, 2, 68)\n",
      "FlattenConsecutive : (4, 136)\n",
      "Linear : (4, 68)\n",
      "BatchNorm1d : (4, 68)\n",
      "Tanh : (4, 68)\n",
      "Linear : (4, 27)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0f304d2a-1f14-4bcd-b1d4-c643d8071801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].out.shape # Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7a878c04-cb3c-49cc-a33c-99bef324ec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 20])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].out.shape # Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0874e3de-12af-4334-925e-ffe1a774c361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 68])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].out.shape # Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ccb9bee8-ac6f-4a5a-b6d1-025e53fe4a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape # Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bfb14384-1324-4a4d-a10d-ab82809d466d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 200])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape # Grouping as a bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "20cae6ea-86d3-4891-a7c4-bb218f3494b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 20])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.randn(4, 8, 10)\n",
    "explicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2)\n",
    "explicit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7af1e-20d8-4ac7-8ea3-05b0a5cb96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 / 200000 -> 3.4210\n",
      " 10000 / 200000 -> 2.3347\n",
      " 20000 / 200000 -> 2.1343\n",
      " 30000 / 200000 -> 1.8512\n",
      " 40000 / 200000 -> 2.3485\n",
      " 50000 / 200000 -> 2.3273\n",
      " 60000 / 200000 -> 2.3200\n",
      " 70000 / 200000 -> 2.0197\n",
      " 80000 / 200000 -> 2.0996\n",
      " 90000 / 200000 -> 2.0646\n",
      "100000 / 200000 -> 2.0123\n",
      "110000 / 200000 -> 2.6706\n",
      "120000 / 200000 -> 2.0190\n"
     ]
    }
   ],
   "source": [
    "# Optimisation\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for idx in range(max_steps):\n",
    "    # Minibatch construction\n",
    "    # Sample indexes from X (minibatc/h of 32 examples)\n",
    "    idxs = torch.randint(0, d.X[\"train\"].shape[0], (32,))\n",
    "    \n",
    "    # Forward pass, only with the minibatch\n",
    "    x_in = d.X[\"train\"][idxs]\n",
    "    logits = model(x_in)\n",
    "    # Compute the loss\n",
    "    loss = F.cross_entropy(logits, d.Y[\"train\"][idxs])\n",
    "\n",
    "    # Backward pass\n",
    "    # Reset the gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    # Compute the backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradually increase the learning rate in each step\n",
    "    # lr = lrs[idx]\n",
    "    lr = 0.1 if idx < 150000 else 0.01\n",
    "    # Update / nudge the value in the direction of the gradient\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # Each 10k steps print the progress of the loss\n",
    "    if idx % 10000 == 0:\n",
    "        print(f\"{idx:6d} / {max_steps:6d} -> {loss.item():.4f}\")\n",
    "    losses.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa50f45-a4b1-41c7-a561-ca8c82680f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48e054-b804-4c92-9be7-b11ab7b3030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(losses).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e6814e1-a59f-4552-953a-f9d788681f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60713201-c0ed-41fb-89ab-5b594a6e0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, 2.0554397106170654\n",
      "valid, 2.1073784828186035\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(split='train'):\n",
    "    # Loss over the entire training set\n",
    "    x = d.X[split]\n",
    "    logits = model(x)\n",
    "    # Compute the loss\n",
    "    loss = F.cross_entropy(logits, d.Y[split])\n",
    "    \n",
    "    print(f\"{split}, {loss.item()}\")\n",
    "\n",
    "compute_loss()\n",
    "compute_loss('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48ae7bb9-6904-4500-a525-ae579774c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traena.\n",
      "maretchaze.\n",
      "carric.\n",
      "mykon.\n",
      "devanne.\n",
      "shau.\n",
      "copen.\n",
      "maa.\n",
      "zekaisheney.\n",
      "viyah.\n",
      "aubrance.\n",
      "kyrie.\n",
      "sharius.\n",
      "brahmani.\n",
      "gan.\n",
      "chreina.\n",
      "bennalesatareyn.\n",
      "amonttielynne.\n",
      "mae.\n",
      "ogontan.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the model\n",
    "\n",
    "for i in range(20):\n",
    "    # Storage for the characters\n",
    "    out = []\n",
    "    # initial context\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Sample from the probabilities\n",
    "        idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [idx]\n",
    "        out.append(idx)\n",
    "        if idx == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join([d.itos[o] for o in out]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
